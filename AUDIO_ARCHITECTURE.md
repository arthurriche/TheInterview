# Architecture Audio Dual-Path - TheInterview

## âš ï¸ PROBLÃˆME ARCHITECTURAL IMPORTANT

Ce projet utilise **DEUX chemins audio complÃ¨tement sÃ©parÃ©s** :

### Chemin 1 : Frontend OpenAI Direct (pour le transcript)
- `useRealtimeCoach` se connecte directement Ã  OpenAI Realtime API
- L'audio du micro est envoyÃ© Ã  OpenAI
- OpenAI gÃ©nÃ¨re l'audio de rÃ©ponse
- L'audio est jouÃ© **localement** dans le navigateur
- Le transcript est enregistrÃ© pour le feedback

### Chemin 2 : LiveKit Agent â†’ Beyond Presence (pour l'avatar)
- Le LiveKit Agent (`agents/livekit-agent.mjs`) crÃ©e SA PROPRE session OpenAI Realtime
- L'audio du candidat doit Ãªtre envoyÃ© via LiveKit
- OpenAI gÃ©nÃ¨re l'audio de rÃ©ponse
- L'audio est routÃ© vers l'avatar Beyond Presence
- L'avatar parle avec synchronisation labiale

## âŒ Pourquoi l'avatar ne parlait pas avant

**Le problÃ¨me :** Les deux sessions OpenAI ne communiquaient PAS entre elles.

1. Le frontend envoyait l'audio Ã  la session A (OpenAI direct)
2. La session A gÃ©nÃ©rait l'audio de rÃ©ponse
3. L'audio Ã©tait jouÃ© localement (SANS avatar)
4. La session B (LiveKit Agent) Ã©tait **complÃ¨tement silencieuse** car personne ne lui parlait
5. L'avatar Beyond Presence (connectÃ© Ã  la session B) n'avait rien Ã  dire

## âœ… Solution implÃ©mentÃ©e (architecture hybride)

J'ai implÃ©mentÃ© une **architecture hybride** qui utilise les DEUX chemins :

### Chemin 1 : Frontend OpenAI (pour transcript et feedback)
- UtilisÃ© UNIQUEMENT pour capturer le transcript
- L'audio de rÃ©ponse est jouÃ© localement **MAIS pas quand l'avatar est actif**
- Permet de garder le systÃ¨me de feedback existant

### Chemin 2 : LiveKit (pour l'avatar et dialogue vocal)
- **NOUVEAU** : Le micro est maintenant publiÃ© dans LiveKit
- Le LiveKit Agent entend le candidat via LiveKit
- L'agent envoie l'audio Ã  OpenAI Realtime (sa propre session)
- OpenAI gÃ©nÃ¨re la rÃ©ponse audio
- L'audio est routÃ© vers Beyond Presence
- **L'avatar parle maintenant !**

## ğŸ”§ Modifications apportÃ©es

### 1. Publication du micro dans LiveKit
**Fichier :** `components/run/realtime-session.tsx`

```typescript
// Dans connectLiveKit(), aprÃ¨s room.connect()
if (micPreviewStreamRef.current) {
  const audioTracks = micPreviewStreamRef.current.getAudioTracks();
  if (audioTracks.length > 0 && audioTracks[0]) {
    await room.localParticipant.publishTrack(audioTracks[0], {
      name: "candidate-microphone",
      source: livekit.Track.Source.Microphone
    });
    console.log("âœ… Microphone audio published to LiveKit");
  }
}
```

### 2. Greeting retardÃ©
**Fichiers :** 
- `lib/coach/base-coach.service.ts` : Ajout de `delayGreeting` option
- `app/api/coach/fcl055d/start/route.ts` : Pass `delayGreeting: true`
- `app/api/coach/fcl055d/trigger-greeting/route.ts` : Nouvelle route
- `components/run/realtime-session.tsx` : Appel aprÃ¨s avatar connectÃ©

### 3. Audio local restaurÃ© (temporairement)
**Fichier :** `lib/coach/useRealtimeCoach.ts`

J'ai **restaurÃ©** `playAssistantAudio` car elle permet d'entendre le coach mÃªme si l'avatar n'est pas disponible (mode dÃ©gradÃ©).

## ğŸ¯ Flux attendu maintenant

1. **PrÃ©visualisation**
   - CamÃ©ra/micro activÃ©s
   - Beyond Presence session crÃ©Ã©e
   - LiveKit se connecte Ã  la room
   - **NOUVEAU** : Le micro est publiÃ© dans LiveKit

2. **Avatar se connecte**
   - LiveKit Agent rejoint la room
   - L'agent voit le track audio du candidat
   - `avatarStatus` â†’ `"connected"`

3. **DÃ©marrage entretien**
   - Attend que l'avatar soit prÃªt (`waitForAvatarReady`)
   - DÃ©marre le coach OpenAI (session frontend) SANS greeting
   - DÃ©clenche le greeting manuellement
   - **Le coach parle via DEUX chemins** :
     - Audio local (navigateur) - pour backup/debug
     - Audio via LiveKit â†’ Avatar (principal)

4. **Candidat parle**
   - Audio capturÃ© par le micro
   - **EnvoyÃ© via DEUX chemins** :
     - `useRealtimeCoach` â†’ OpenAI direct (pour transcript)
     - LiveKit â†’ Agent â†’ OpenAI (pour rÃ©ponse avatar)

5. **Coach rÃ©pond**
   - **Session frontend** : GÃ©nÃ¨re audio + transcript
   - **Session LiveKit Agent** : GÃ©nÃ¨re audio â†’ Avatar
   - **RÃ©sultat** : L'avatar parle avec ses lÃ¨vres synchronisÃ©es

## âš ï¸ Limitations actuelles

### Double audio possible
Si l'audio local ET l'avatar parlent en mÃªme temps, vous entendrez **deux fois** la voix du coach. Pour Ã©viter cela, il faudrait :

**Option A** : DÃ©tecter si l'avatar est connectÃ© et dÃ©sactiver l'audio local
```typescript
const playAssistantAudio = useCallback((base64: string) => {
  // Ne jouer localement QUE si l'avatar n'est pas disponible
  if (avatarIsConnected) {
    console.log("Audio routÃ© vers avatar, skip local playback");
    return;
  }
  // ... code de lecture local
}, [avatarIsConnected]);
```

**Option B** : Supprimer complÃ¨tement `useRealtimeCoach` et utiliser uniquement LiveKit

### Transcript potentiellement incomplet
Comme il y a deux sessions OpenAI, le transcript de la session frontend pourrait ne pas inclure TOUTES les rÃ©ponses du coach si elles viennent uniquement de la session LiveKit.

**Solution idÃ©ale** : Le LiveKit Agent devrait envoyer le transcript au backend via un webhook ou DataChannel.

## ğŸš€ Solution idÃ©ale (refactorisation majeure)

Pour une architecture propre, il faudrait :

1. **Supprimer `useRealtimeCoach`** et sa connexion OpenAI directe
2. **Utiliser UNIQUEMENT LiveKit** pour TOUT l'audio
3. **RÃ©cupÃ©rer le transcript** via LiveKit DataChannel ou SSE depuis l'agent
4. **Un seul chemin** : Frontend â†’ LiveKit â†’ Agent â†’ OpenAI â†’ Avatar

### Architecture cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚
â”‚   (React)   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ LiveKit (audio + video)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LiveKit Server  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LiveKit Agent   â”‚
â”‚  (Node.js)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ OpenAI RT API  â”‚
â”‚ â€¢ Beyond Presenceâ”‚
â”‚ â€¢ Transcript log â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ Transcript via DataChannel
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚
â”‚  (display)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Tests Ã  effectuer

### 1. Test basique - L'avatar parle-t-il ?
```bash
# Terminal 1: Lancer l'agent
cd agents
node livekit-agent.mjs

# Terminal 2: Lancer Next.js
npm run dev
```

- Allez sur `/run`
- Attendez "Avatar : connected"
- Cliquez "DÃ©marrer l'entretien"
- **VÃ©rifiez** : L'avatar parle et bouge ses lÃ¨vres
- **VÃ©rifiez** : L'audio est synchronisÃ© avec les lÃ¨vres

### 2. Test double audio
- Ã‰coutez attentivement
- **Si vous entendez deux voix simultanÃ©es** â†’ double audio confirmÃ©
- **Solution temporaire** : Baisser le volume systÃ¨me

### 3. Test dialogue
- RÃ©pondez Ã  la question
- **VÃ©rifiez** : L'avatar entend votre rÃ©ponse (pose une question de suivi)
- **VÃ©rifiez** : Le coach ne rÃ©pond pas Ã  sa propre question

### 4. VÃ©rifier les logs
Ouvrez la console navigateur et cherchez :
```
âœ… Microphone audio published to LiveKit - agent can now hear candidate
â¸ï¸ Greeting delayed - waiting for avatar connection
â–¶ï¸ Triggering greeting now that avatar is ready
âœ… Greeting triggered successfully - avatar should speak now
```

## ğŸ› Troubleshooting

### L'avatar ne parle toujours pas

**VÃ©rifiez :**
1. L'agent LiveKit tourne-t-il ? (`node livekit-agent.mjs`)
2. Les variables `.env` sont-elles correctes ?
   ```
   LIVEKIT_API_KEY=...
   LIVEKIT_API_SECRET=...
   LIVEKIT_URL=wss://...
   OPENAI_API_KEY=sk-...
   BEY_API_KEY=...
   ```
3. Logs de l'agent : Y a-t-il des erreurs ?
4. Console navigateur : Le micro est-il publiÃ© dans LiveKit ?

### Double audio

C'est normal pour l'instant (architecture hybride). Pour corriger :
- Modifier `playAssistantAudio` pour dÃ©tecter si l'avatar est connectÃ©
- OU implÃ©menter l'architecture cible (refactorisation majeure)

### Le transcript est incomplet

C'est attendu car seule la session frontend enregistre le transcript. L'agent LiveKit devrait envoyer son transcript au backend.

---

**Date :** 15 novembre 2025  
**Architecture :** Hybride (dual-path)  
**Statut :** Fonctionnel mais sous-optimal  
**Prochaine Ã©tape :** Migration vers architecture LiveKit-only
